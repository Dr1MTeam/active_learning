{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from base_trainer import Trainer\n",
    "from model import SimpleCNN\n",
    "from torch.utils.data import DataLoader, Subset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveTrainer(Trainer):\n",
    "    def select_samples(self, dataloader, num_samples_to_select):\n",
    "        \"\"\"\n",
    "        Выбираем образцы на основе наименьшей уверенности модели.\n",
    "        \n",
    "        :param dataloader: Загрузчик данных для выборки\n",
    "        :param num_samples_to_select: Количество образцов для выбора\n",
    "        :return: Список индексов выбранных образцов\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Устанавливаем модель в режим оценки\n",
    "        all_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in dataloader:\n",
    "                inputs = inputs.to(self.device)  # Переносим данные на устройство модели\n",
    "                outputs = self.model(inputs)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence_scores, _ = torch.max(probabilities, dim=1)  # Максимальная вероятность для каждого примера\n",
    "                all_confidences.append(confidence_scores)\n",
    "\n",
    "        # Конкатенируем все уверенности в один тензор\n",
    "        confidence_scores = torch.cat(all_confidences)\n",
    "        \n",
    "        # Находим индексы образцов с наименьшей уверенностью\n",
    "        least_confident_indices = torch.argsort(1 - confidence_scores)[:num_samples_to_select]\n",
    "        self.update_dataloader(least_confident_indices.cpu().numpy())\n",
    "        print(\"watch = \", least_confident_indices.cpu().numpy())\n",
    "        return least_confident_indices.cpu().numpy()\n",
    "    \n",
    "    def fit(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Полный цикл обучения.\n",
    "        :param num_epochs: Количество эпох\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_step()\n",
    "            val_loss, accuracy, f1 = self.val_step()\n",
    "            self.select_samples(dataloader = self.pool_loader, num_samples_to_select = num_epochs)\n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initial dataset (1%): 500\n",
      "Initial dataset (10%): 5000\n",
      "Initial dataset (20%): 10000\n",
      "Pool data size: 35698\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10           # CIFAR-10\n",
    "NUM_EPOCH = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "percentages = [0.01, 0.10, 0.20]\n",
    "initial_datasets = {}\n",
    "pool_data_indices = []\n",
    "\n",
    "# Создаем словарь для хранения индексов по классам\n",
    "class_indices = defaultdict(list)\n",
    "\n",
    "# Заполняем словарь индексами изображений по классам\n",
    "for index, (_, label) in enumerate(train_dataset):\n",
    "    class_indices[label].append(index)\n",
    "\n",
    "# Формируем начальные наборы данных\n",
    "for percentage in percentages:\n",
    "    initial_indices = []\n",
    "    num_samples_per_class = {label: int(len(indices) * percentage) for label, indices in class_indices.items()}\n",
    "    \n",
    "    for label, indices in class_indices.items():\n",
    "        # Случайным образом выбираем индексы для каждого класса\n",
    "        selected_indices = np.random.choice(indices, num_samples_per_class[label], replace=False)\n",
    "        initial_indices.extend(selected_indices)\n",
    "    \n",
    "    # Сортируем индексы для создания подмножества\n",
    "    initial_datasets[percentage] = sorted(initial_indices)\n",
    "\n",
    "# Создаем pool_data с оставшимися данными\n",
    "all_initial_indices = set(initial_datasets[0.01] + initial_datasets[0.10] + initial_datasets[0.20])\n",
    "pool_data_indices = [i for i in range(len(train_dataset)) if i not in all_initial_indices]\n",
    "\n",
    "# Создаем подмножества для начальных данных и оставшихся данных\n",
    "initial_dataset_1_percent = torch.utils.data.Subset(train_dataset, initial_datasets[0.01])\n",
    "initial_dataset_10_percent = torch.utils.data.Subset(train_dataset, initial_datasets[0.10])\n",
    "initial_dataset_20_percent = torch.utils.data.Subset(train_dataset, initial_datasets[0.20])\n",
    "pool_data = torch.utils.data.Subset(train_dataset, pool_data_indices)\n",
    "\n",
    "# Проверяем размеры подмножеств\n",
    "print(f\"Initial dataset (1%): {len(initial_dataset_1_percent)}\")\n",
    "print(f\"Initial dataset (10%): {len(initial_dataset_10_percent)}\")\n",
    "print(f\"Initial dataset (20%): {len(initial_dataset_20_percent)}\")\n",
    "print(f\"Pool data size: {len(pool_data)}\")\n",
    "\n",
    "train_dataloader = DataLoader(initial_dataset_1_percent, batch_size=64, shuffle=True)\n",
    "pool_dataloader = DataLoader(pool_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "model = SimpleCNN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train = ActiveTrainer(model=model.to(DEVICE), optimizer=optimizer,criterion=criterion, train_loader=train_dataloader, val_loader=val_dataloader,pool_loader = pool_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:00<00:00, 18.43it/s]\n",
      "Validating: 100%|██████████| 157/157 [00:01<00:00, 81.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2381, Accuracy: 0.1399, F1 Score: 0.0741\n",
      "35698\n",
      "35696\n",
      "watch =  [21219  1797]\n",
      "Epoch 1/2 - Train Loss: 2.3160, Val Loss: 2.2381, Acc: 0.1399, F1: 0.0741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [00:00<00:00, 59.70it/s]\n",
      "Validating: 100%|██████████| 157/157 [00:02<00:00, 74.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1598, Accuracy: 0.2267, F1 Score: 0.1657\n",
      "35696\n",
      "35694\n",
      "watch =  [14922 13583]\n",
      "Epoch 2/2 - Train Loss: 2.1588, Val Loss: 2.1598, Acc: 0.2267, F1: 0.1657\n"
     ]
    }
   ],
   "source": [
    "train.fit(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
